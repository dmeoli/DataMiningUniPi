\section{Clustering}
We now run and compare some clustering algorithm in order to find some structures among the data. First we start with a basic \emph{K-Means} followed with some \emph{Hierarchical clustering techniques} and \emph{DBSCAN}. 
\subsection{Preprocessing}

\begin{wrapfigure}[10]{r}{0.4\textwidth}
\vspace{-13mm}
\centering
\captionsetup{justification=centering}
\includegraphics[width=.4\textwidth]{img/clustering/pca.png}
\caption{Explained variance ratio}
\label{fig:pca_img}
\end{wrapfigure}

The data matrix was first standardized and then the two principal components were extracted. The choice was between two and tree principal component since they retain respectively $\sim$0.75 and $\sim$0.89 of the variance of the data.

We selected the two principal components, in this way, we could also have a visual inspection.

\subsection{K-Means}
The algorithm run for $K$ ranging from \emph{2} to \emph{15} clusters. For each iteration the SSE and the average silhouette value were computed.

\begin{figure}[h!]
     \captionsetup{justification=centering}		
     \centering
     \begin{subfigure}{0.49\textwidth}
         \centering
	 \captionsetup{type=figure}
         \includegraphics[width=\textwidth]{img/clustering/sse.png}
         \caption{SSE}
         \label{fig:sse_img}
     \end{subfigure}
     \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/clustering/sil.png}
         \caption{Average silhouette}
         \label{fig:sil_img}
     \end{subfigure}
     \caption{\emph{K-Means} metrics}
    \label{fig:km_metrics}
\end{figure}

By looking at the plots in Figure \ref{fig:km_metrics}, we can see that there isn't a prominent \emph{elbow shape}, so we needed the analysis of more metrics to get the optimal number of clusters. By inspecting the silhouette score, it is possible to see that it has its maximum for $K = 2$, followed by $K = 3$. In conclusion, we opted for 3 clusters, to get a trade off between high silhouette and low SSE.

\begin{wrapfigure}[12]{l}{.4\textwidth}
    \vspace{-5mm}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=.4\textwidth]{img/clustering/sil_tot.png}
    \caption{Silhouette score\\ for each data point}
    \label{fig:silhouette}
\end{wrapfigure}

The resulting clusters have a comparable number of points. In particular \emph{Cluster 0} has \textbf{2006} points, \emph{Cluster 1} \textbf{1440} and \emph{Cluster 2} \textbf{760}.\\
In Figure \ref{fig:silhouette}, we can see the plot of the Silhouette score, where each different color represents a different cluster, and the dotted red line is the average silhouette score. In particular we have that \emph{Cluster 0} has a silhouette of \emph{0.39}, \emph{0.40} for \emph{Cluster 1} and \emph{0.35} for \emph{Cluster 2}, with an overall average silhouette of \emph{0.39}. We see that a small fraction of points in \emph{Cluster 0} have a negative value but overall the Silhouette suggests a good clustering.

\begin{figure}[h!]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/clustering/km_clusters.png}
        \caption{Clustering result}
        \label{fig:skmclust}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/clustering/sim_heatmap.png}
        \caption{Similarity heatmap}
        \label{fig:sim_heatmap}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{img/clustering/cluster_avg.png}
	\caption{Average values\\ per cluster}
        \label{fig:km_avg}
    \end{subfigure}
\end{figure}

In Figure \ref{fig:skmclust}, we can appreciate the results of the algorithm, where we can see that the clusters are well separated from one another.\\
The similarity matrix (Figure \ref{fig:sim_heatmap}) shows the affinity of elements inside a cluster; we can see that the results are pretty good.\\
The plot in Figure \ref{fig:km_avg} shows the average values for each attribute divided by the clusters. In particular, it is possible to see that \emph{Cluster 1} contains the most frequent and spending customers, followed by \emph{Cluster 1} and \emph{Cluster 2}.

\subsection{Hierarchical clustering}
We used different kinds of algorithm: \emph{Complete Link}, \emph{Single Link}, \emph{Ward} and \emph{Average Link}. By looking at the dendograms in Figure \ref{fig:dendograms}, if we cut the tree by selecting two or three clusters, we can see that \emph{Single Link} and \emph{Average Link} generate an unbalanced clustering leaving the last merges between a large cluster and a small one, in some cases even a singleton. The most balanced are obtain with \emph{Ward} and \emph{Complete Link}.

\subsection{DBSCAN}

We explored different combination of \emph{eps} for a given \emph{MinPts}; to choose \emph{eps} we checked the \emph{KNN} distance, with $K$ equal to \emph{MinPts}.\\
If \emph{MinPts} is $20$, the optimal \emph{eps} is around $0.25$. In this case the algorithm found a large dense area surrounded by noise points, as is showed in Figure \ref{fig:dbscan}. By increasing \emph{eps} to $0.3$ the results are the same, if instead the value of $0.2$ is used the algorithm finds more clusters of negligible dimension.\\

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{img/clustering/c_link.png}
        \caption{\emph{Complete Link}}
        \label{fig:clink_img}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{img/clustering/s_link.png}
        \caption{\emph{Single Link}}
        \label{fig:slink_img}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{img/clustering/ward.png}
        \caption{\emph{Ward}}
        \label{fig:ward_img}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
         \centering
         \includegraphics[width=0.7\textwidth]{img/clustering/centroid.png}
         \caption{\emph{Centroids}}
         \label{fig:centr_img}
     \end{subfigure}
     \caption{Dendograms}
    \label{fig:dendograms}
\end{figure}

\begin{figure}[h!]
	\captionsetup{justification=centering}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=.65\textwidth]{img/clustering/dbscan.png}
		\centering
		\label{fig:dbscan_good}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=.65\textwidth]{img/clustering/dbscan_bad.png}
		\centering
		\label{fig:dbscan_bad}
	\end{subfigure}
	\caption{Results of DBSCAN}
	\label{fig:dbscan}
\end{figure}

\subsection{Clustering results}

In conclusion we opted for the \emph{K-Means} clustering to characterize the customers. In fact it is able to characterize the customer based on its purchasing behavior as in seen in Figure \ref{fig:pairplot}.

\begin{figure}[h!]
	\captionsetup{justification=centering}
	\centering
	\includegraphics[width=\textwidth]{img/clustering/pair_plot_clust.png}
	\centering
	\caption{K-Means results}
	\label{fig:pairplot}
\end{figure}


